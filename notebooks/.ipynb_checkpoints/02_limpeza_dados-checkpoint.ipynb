{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72fecb7a-4bd8-45f7-b3d9-7e94e08f9f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MÓDULO DE LIMPEZA E VALIDAÇÃO\n",
      "==================================================\n",
      "PROCESSAMENTO DE DADOS - MÓDULO DE ETL\n",
      "==================================================\n",
      "Utilizando dataset sintético para desenvolvimento e testes...\n",
      "\n",
      "Iniciando processamento de 4 tabelas...\n",
      "\n",
      "Tabela: transacoes\n",
      "   Dimensões iniciais: (5000, 5)\n",
      "   Processando: data_transacao\n",
      "   Tipo atual: datetime64[ns]\n",
      "   Status: Formato datetime já aplicado\n",
      "   Dimensões finais: (5000, 5)\n",
      "\n",
      "Tabela: contas\n",
      "   Dimensões iniciais: (100, 4)\n",
      "   Processando: data_abertura\n",
      "   Tipo atual: datetime64[ns]\n",
      "   Status: Formato datetime já aplicado\n",
      "   Dimensões finais: (100, 4)\n",
      "\n",
      "Tabela: agencias\n",
      "   Dimensões iniciais: (10, 3)\n",
      "   Dimensões finais: (10, 3)\n",
      "\n",
      "Tabela: clientes\n",
      "   Dimensões iniciais: (100, 4)\n",
      "   Processando: data_nascimento\n",
      "   Tipo atual: datetime64[ns]\n",
      "   Status: Formato datetime já aplicado\n",
      "   Dimensões finais: (100, 4)\n",
      "\n",
      "==================================================\n",
      "PROCESSAMENTO CONCLUÍDO\n",
      "==================================================\n",
      "\n",
      "Validação: Dataset transacional aprovado para análise\n",
      "Período coberto: 2022-01-01 00:00:00 até 2023-02-21 14:00:00\n",
      "Volume de transações: 5,000\n",
      "\n",
      "SUCESSO: 4 tabelas processadas e validadas\n",
      "\n",
      "Variável 'dados' disponibilizada para módulos subsequentes\n",
      "Pipeline pronto para fase de análise exploratória\n",
      "\n",
      "MÓDULO DE LIMPEZA E VALIDAÇÃO\n",
      "==================================================\n",
      "PROCESSAMENTO DE DADOS - MÓDULO DE ETL\n",
      "==================================================\n",
      "Utilizando dataset sintético para desenvolvimento e testes...\n",
      "\n",
      "Iniciando processamento de 4 tabelas...\n",
      "\n",
      "Tabela: transacoes\n",
      "   Dimensões iniciais: (5000, 5)\n",
      "   Processando: data_transacao\n",
      "   Tipo atual: datetime64[ns]\n",
      "   Status: Formato datetime já aplicado\n",
      "   Dimensões finais: (5000, 5)\n",
      "\n",
      "Tabela: contas\n",
      "   Dimensões iniciais: (100, 4)\n",
      "   Processando: data_abertura\n",
      "   Tipo atual: datetime64[ns]\n",
      "   Status: Formato datetime já aplicado\n",
      "   Dimensões finais: (100, 4)\n",
      "\n",
      "Tabela: agencias\n",
      "   Dimensões iniciais: (10, 3)\n",
      "   Dimensões finais: (10, 3)\n",
      "\n",
      "Tabela: clientes\n",
      "   Dimensões iniciais: (100, 4)\n",
      "   Processando: data_nascimento\n",
      "   Tipo atual: datetime64[ns]\n",
      "   Status: Formato datetime já aplicado\n",
      "   Dimensões finais: (100, 4)\n",
      "\n",
      "==================================================\n",
      "PROCESSAMENTO CONCLUÍDO\n",
      "==================================================\n",
      "\n",
      "Validação: Dataset transacional aprovado para análise\n",
      "Período coberto: 2022-01-01 00:00:00 até 2023-02-21 14:00:00\n",
      "Volume de transações: 5,000\n",
      "\n",
      "Dados integrados ao ambiente Jupyter (variável 'dados')\n"
     ]
    }
   ],
   "source": [
    "# 02_LIMPEZA_DADOS.PY - PROCESSAMENTO E VALIDAÇÃO DE DADOS\n",
    "# ========================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def carregar_dados():\n",
    "    \"\"\"\n",
    "    Carrega dados de múltiplas fontes com fallback automático\n",
    "    \n",
    "    Estratégia de carregamento:\n",
    "    1. Arquivos ZIP compactados\n",
    "    2. CSVs individuais no diretório\n",
    "    3. Dataset sintético para testes\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dicionário com DataFrames das tabelas\n",
    "    \"\"\"\n",
    "    print(\"PROCESSAMENTO DE DADOS - MÓDULO DE ETL\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Tentativa 1: Carregar de arquivos ZIP\n",
    "    arquivos_zip = ['banvic_data.zip', 'dados.zip', 'data.zip']\n",
    "    \n",
    "    for arquivo_zip in arquivos_zip:\n",
    "        if os.path.exists(arquivo_zip):\n",
    "            try:\n",
    "                dados = {}\n",
    "                with zipfile.ZipFile(arquivo_zip, 'r') as zip_ref:\n",
    "                    for arquivo in zip_ref.namelist():\n",
    "                        if arquivo.endswith('.csv'):\n",
    "                            nome_tabela = arquivo.replace('.csv', '').replace('/', '')\n",
    "                            dados[nome_tabela] = pd.read_csv(zip_ref.open(arquivo))\n",
    "                \n",
    "                print(f\"Fonte de dados: {arquivo_zip}\")\n",
    "                print(f\"Tabelas carregadas: {list(dados.keys())}\")\n",
    "                return dados\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Falha no carregamento de {arquivo_zip}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    # Tentativa 2: CSVs individuais no diretório atual\n",
    "    arquivos_csv = ['transacoes.csv', 'clientes.csv', 'contas.csv', 'agencias.csv']\n",
    "    arquivos_encontrados = [arq for arq in arquivos_csv if os.path.exists(arq)]\n",
    "    \n",
    "    if arquivos_encontrados:\n",
    "        try:\n",
    "            dados = {}\n",
    "            for arquivo in arquivos_encontrados:\n",
    "                nome_tabela = arquivo.replace('.csv', '')\n",
    "                dados[nome_tabela] = pd.read_csv(arquivo)\n",
    "            \n",
    "            print(f\"CSVs processados: {arquivos_encontrados}\")\n",
    "            return dados\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erro no processamento dos CSVs: {e}\")\n",
    "    \n",
    "    # Fallback: Dataset sintético para desenvolvimento\n",
    "    print(\"Utilizando dataset sintético para desenvolvimento e testes...\")\n",
    "    return gerar_dataset_sintetico()\n",
    "\n",
    "def gerar_dataset_sintetico():\n",
    "    \"\"\"\n",
    "    Gera dataset sintético baseado no modelo de dados bancários\n",
    "    Utilizado quando dados reais não estão disponíveis\n",
    "    \"\"\"\n",
    "    np.random.seed(42)  # Seed fixo para reprodutibilidade\n",
    "    \n",
    "    # Geração da tabela de transações\n",
    "    n_transacoes = 5000\n",
    "    data_base = datetime(2022, 1, 1)\n",
    "    \n",
    "    transacoes = pd.DataFrame({\n",
    "        'cod_transacao': range(1, n_transacoes + 1),\n",
    "        'data_transacao': pd.date_range(data_base, periods=n_transacoes, freq='2H'),\n",
    "        'valor_transacao': np.random.uniform(10, 5000, n_transacoes),\n",
    "        'nome_transacao': np.random.choice([\n",
    "            'Transferencia', 'Deposito', 'Saque', 'Pagamento', \n",
    "            'TED', 'DOC', 'Pix'\n",
    "        ], n_transacoes),\n",
    "        'num_conta': np.random.randint(1000, 1100, n_transacoes)\n",
    "    })\n",
    "    \n",
    "    # Tabela de contas correntes\n",
    "    contas = pd.DataFrame({\n",
    "        'num_conta': range(1000, 1100),\n",
    "        'cod_agencia': np.random.randint(1, 11, 100),\n",
    "        'saldo_atual': np.random.uniform(100, 50000, 100),\n",
    "        'data_abertura': pd.date_range('2020-01-01', periods=100, freq='3D')\n",
    "    })\n",
    "    \n",
    "    # Cadastro de agências\n",
    "    agencias = pd.DataFrame({\n",
    "        'cod_agencia': range(1, 11),\n",
    "        'nome_agencia': [f'Agencia_{i}' for i in range(1, 11)],\n",
    "        'cidade': np.random.choice(['São Paulo', 'Rio de Janeiro', 'Belo Horizonte'], 10)\n",
    "    })\n",
    "    \n",
    "    # Base de clientes\n",
    "    clientes = pd.DataFrame({\n",
    "        'cod_cliente': range(1, 101),\n",
    "        'nome_cliente': [f'Cliente_{i}' for i in range(1, 101)],\n",
    "        'idade': np.random.randint(18, 80, 100),\n",
    "        'data_nascimento': pd.date_range('1943-01-01', '2005-01-01', periods=100)\n",
    "    })\n",
    "    \n",
    "    return {\n",
    "        'transacoes': transacoes,\n",
    "        'contas': contas,\n",
    "        'agencias': agencias,\n",
    "        'clientes': clientes\n",
    "    }\n",
    "\n",
    "def processar_coluna_temporal(df, nome_coluna):\n",
    "    \"\"\"\n",
    "    Padroniza e valida colunas com dados temporais\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame de origem\n",
    "        nome_coluna: Nome da coluna a ser processada\n",
    "    \n",
    "    Returns:\n",
    "        bool: Status da operação (True=sucesso, False=falha)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"   Processando: {nome_coluna}\")\n",
    "        print(f\"   Tipo atual: {df[nome_coluna].dtype}\")\n",
    "        \n",
    "        # Verificar se já está no formato correto\n",
    "        if pd.api.types.is_datetime64_any_dtype(df[nome_coluna]):\n",
    "            print(f\"   Status: Formato datetime já aplicado\")\n",
    "            return True\n",
    "        \n",
    "        # Aplicar conversão para datetime\n",
    "        df[nome_coluna] = pd.to_datetime(df[nome_coluna], errors='coerce')\n",
    "        \n",
    "        # Validar resultado da conversão\n",
    "        registros_validos = df[nome_coluna].notna().sum()\n",
    "        if registros_validos > 0:\n",
    "            print(f\"   Status: Conversão realizada ({registros_validos} registros válidos)\")\n",
    "            data_min = df[nome_coluna].min()\n",
    "            data_max = df[nome_coluna].max()\n",
    "            print(f\"   Intervalo temporal: {data_min} até {data_max}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"   Erro: Conversão resultou em dados inválidos\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   Erro crítico na conversão: {e}\")\n",
    "        return False\n",
    "\n",
    "def executar_limpeza_dados():\n",
    "    \"\"\"\n",
    "    Coordena o processo completo de limpeza e validação dos dados\n",
    "    \n",
    "    Operações executadas:\n",
    "    - Carregamento de dados\n",
    "    - Padronização de colunas temporais\n",
    "    - Remoção de registros duplicados\n",
    "    - Análise de qualidade dos dados\n",
    "    - Validação final\n",
    "    \"\"\"\n",
    "    print(\"\\nMÓDULO DE LIMPEZA E VALIDAÇÃO\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Carregar dados das fontes disponíveis\n",
    "    dados = carregar_dados()\n",
    "    \n",
    "    if dados is None:\n",
    "        print(\"ERRO CRÍTICO: Falha no carregamento dos dados\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nIniciando processamento de {len(dados)} tabelas...\")\n",
    "    \n",
    "    # Processar cada tabela individualmente\n",
    "    for nome_tabela, df in dados.items():\n",
    "        print(f\"\\nTabela: {nome_tabela}\")\n",
    "        print(f\"   Dimensões iniciais: {df.shape}\")\n",
    "        \n",
    "        # Identificar e processar colunas temporais\n",
    "        colunas_temporais = [col for col in df.columns if 'data' in col.lower()]\n",
    "        if colunas_temporais:\n",
    "            for coluna in colunas_temporais:\n",
    "                processar_coluna_temporal(df, coluna)\n",
    "        \n",
    "        # Remover registros duplicados\n",
    "        registros_iniciais = df.shape[0]\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        registros_finais = df.shape[0]\n",
    "        duplicatas_removidas = registros_iniciais - registros_finais\n",
    "        \n",
    "        if duplicatas_removidas > 0:\n",
    "            print(f\"   Duplicatas eliminadas: {duplicatas_removidas}\")\n",
    "        \n",
    "        # Análise de completude dos dados\n",
    "        valores_nulos = df.isnull().sum().sum()\n",
    "        if valores_nulos > 0:\n",
    "            print(f\"   Valores missing detectados: {valores_nulos}\")\n",
    "        \n",
    "        print(f\"   Dimensões finais: {df.shape}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PROCESSAMENTO CONCLUÍDO\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Validação específica para dados transacionais\n",
    "    if 'transacoes' in dados:\n",
    "        trans = dados['transacoes']\n",
    "        if 'data_transacao' in trans.columns:\n",
    "            if pd.api.types.is_datetime64_any_dtype(trans['data_transacao']):\n",
    "                print(\"\\nValidação: Dataset transacional aprovado para análise\")\n",
    "                print(f\"Período coberto: {trans['data_transacao'].min()} até {trans['data_transacao'].max()}\")\n",
    "                print(f\"Volume de transações: {len(trans):,}\")\n",
    "            else:\n",
    "                print(\"\\nAtenção: Dados transacionais necessitam de tratamento adicional\")\n",
    "    \n",
    "    return dados\n",
    "\n",
    "# Execução principal do módulo\n",
    "if __name__ == \"__main__\":\n",
    "    # Executar pipeline de limpeza\n",
    "    dados_processados = executar_limpeza_dados()\n",
    "    \n",
    "    if dados_processados:\n",
    "        print(f\"\\nSUCESSO: {len(dados_processados)} tabelas processadas e validadas\")\n",
    "        \n",
    "        # Disponibilizar dados no namespace global\n",
    "        globals()['dados'] = dados_processados\n",
    "        \n",
    "        print(\"\\nVariável 'dados' disponibilizada para módulos subsequentes\")\n",
    "        print(\"Pipeline pronto para fase de análise exploratória\")\n",
    "    else:\n",
    "        print(\"\\nFALHA: Pipeline de processamento interrompido\")\n",
    "\n",
    "# Compatibilidade com ambiente Jupyter\n",
    "try:\n",
    "    if 'get_ipython' in globals():\n",
    "        dados = executar_limpeza_dados()\n",
    "        if dados:\n",
    "            print(\"\\nDados integrados ao ambiente Jupyter (variável 'dados')\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a79112-970f-47aa-9d3d-02825082adc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
